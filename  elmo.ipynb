{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkushaljain\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2022-09-29 13:32:13.556040: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-29 13:32:13.556112: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kushal/Kushal/7sem/ANLP/ELMo-pytorch/wandb/run-20220929_133207-3tiuixhy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kushaljain/ELMo/runs/3tiuixhy\" target=\"_blank\">lunar-glade-1</a></strong> to <a href=\"https://wandb.ai/kushaljain/ELMo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Formed and saved. The length of dictionary is-:  9773\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from dataset import Dataset_seq,build_vocab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import time\n",
    "from model import Bi_RNN\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"ELMo\", entity=\"kushaljain\")\n",
    "\n",
    "train_path = \"./Dataset/yelp-subset.train.csv\"\n",
    "test_path = \"./Dataset/yelp-subset.test.csv\"\n",
    "val_path = \"./Dataset/yelp-subset.dev.csv\"\n",
    "word2id_ts,id2word_ts = build_vocab(train_path)\n",
    "\n",
    "bs = 100\n",
    "yelp_train = Dataset_seq(word2id_ts,id2word_ts, train_path)\n",
    "yelp_test = Dataset_seq(word2id_ts,id2word_ts, test_path)\n",
    "yelp_val = Dataset_seq(word2id_ts,id2word_ts, val_path)\n",
    "train_dl = DataLoader(yelp_train, shuffle=False, batch_size=bs, num_workers=2)\n",
    "val_dl = DataLoader(yelp_val, shuffle=True, batch_size=bs, num_workers=2)\n",
    "test_dl = DataLoader(yelp_test, shuffle=True, batch_size=bs, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Bi_RNN(50, 100, bs, len(word2id_ts), 2)\n",
    "lr = 0.0003\n",
    "epochs = 100\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = model.float()\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "error = {\"train\": [], \"val\": []}\n",
    "accuracy = {\"train\" : [], \"val\" : []}\n",
    "perplexity = {\"train\": [], \"val\": []}\n",
    "times = {\"train\": [], \"val\": []}\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Epoch : 1\n",
      "Time taken to run training for epoch 0 : 21.97588038444519 \n",
      "Time taken to run training for epoch 0 : 0.5867903232574463 \n",
      "Total Time for epoch 1 is 22.562707901000977\n",
      "Training Loss for epoch 1 : 288.7898107767105 \n",
      "Training Accuracy for epoch 1 : 1880.0040283203125\n",
      "Val Loss for epoch 1 : 24.460302233695984 \n",
      "Val Accuracy for epoch 1 : 1756.115478515625\n",
      "Train perplexity for epoch 1 : 2.6291856315861897e+125\n",
      "Val perplexity for epoch 1 : 41973413114.16328\n",
      "starting Epoch : 2\n",
      "Time taken to run training for epoch 1 : 22.88041591644287 \n",
      "Time taken to run training for epoch 1 : 0.6401140689849854 \n",
      "Total Time for epoch 2 is 23.520609855651855\n",
      "Training Loss for epoch 2 : 278.244054466486 \n",
      "Training Accuracy for epoch 2 : 1883.716552734375\n",
      "Val Loss for epoch 2 : 23.56842663884163 \n",
      "Val Accuracy for epoch 2 : 1762.8846435546875\n",
      "Train perplexity for epoch 2 : 6.916039693662191e+120\n",
      "Val perplexity for epoch 2 : 17204324934.56452\n",
      "starting Epoch : 3\n",
      "Time taken to run training for epoch 2 : 22.699109315872192 \n",
      "Time taken to run training for epoch 2 : 0.5504100322723389 \n",
      "Total Time for epoch 3 is 23.249542713165283\n",
      "Training Loss for epoch 3 : 271.9187670350075 \n",
      "Training Accuracy for epoch 3 : 1886.327392578125\n",
      "Val Loss for epoch 3 : 23.90181005001068 \n",
      "Val Accuracy for epoch 3 : 1764.2308349609375\n",
      "Train perplexity for epoch 3 : 1.2382833950917824e+118\n",
      "Val perplexity for epoch 3 : 24011772071.366043\n",
      "starting Epoch : 4\n",
      "Time taken to run training for epoch 3 : 22.397955417633057 \n",
      "Time taken to run training for epoch 3 : 0.6164760589599609 \n",
      "Total Time for epoch 4 is 23.014509916305542\n",
      "Training Loss for epoch 4 : 270.5634236037731 \n",
      "Training Accuracy for epoch 4 : 1886.490966796875\n",
      "Val Loss for epoch 4 : 25.935302138328552 \n",
      "Val Accuracy for epoch 4 : 1761.423095703125\n",
      "Train perplexity for epoch 4 : 3.1930217435646805e+117\n",
      "Val perplexity for epoch 4 : 183467272737.1633\n",
      "starting Epoch : 5\n",
      "Time taken to run training for epoch 4 : 23.04346752166748 \n",
      "Time taken to run training for epoch 4 : 0.5734622478485107 \n",
      "Total Time for epoch 5 is 23.616952657699585\n",
      "Training Loss for epoch 5 : 260.99822533130646 \n",
      "Training Accuracy for epoch 5 : 1889.2435302734375\n",
      "Val Loss for epoch 5 : 24.47058367729187 \n",
      "Val Accuracy for epoch 5 : 1764.7308349609375\n",
      "Train perplexity for epoch 5 : 2.2391802139792313e+113\n",
      "Val perplexity for epoch 5 : 42407186480.71836\n",
      "Saving the model \n",
      "starting Epoch : 6\n",
      "Time taken to run training for epoch 5 : 24.213197946548462 \n",
      "Time taken to run training for epoch 5 : 0.6089940071105957 \n",
      "Total Time for epoch 6 is 24.822223663330078\n",
      "Training Loss for epoch 6 : 259.4081064462662 \n",
      "Training Accuracy for epoch 6 : 1890.079833984375\n",
      "Val Loss for epoch 6 : 24.723048508167267 \n",
      "Val Accuracy for epoch 6 : 1762.769287109375\n",
      "Train perplexity for epoch 6 : 4.565719121220546e+112\n",
      "Val perplexity for epoch 6 : 54586285573.145\n",
      "starting Epoch : 7\n",
      "Time taken to run training for epoch 6 : 22.65188431739807 \n",
      "Time taken to run training for epoch 6 : 0.5779421329498291 \n",
      "Total Time for epoch 7 is 23.22985053062439\n",
      "Training Loss for epoch 7 : 253.74108439683914 \n",
      "Training Accuracy for epoch 7 : 1891.3114013671875\n",
      "Val Loss for epoch 7 : 23.914742916822433 \n",
      "Val Accuracy for epoch 7 : 1767.6539306640625\n",
      "Train perplexity for epoch 7 : 1.5788933140806516e+110\n",
      "Val perplexity for epoch 7 : 24324329899.361244\n",
      "starting Epoch : 8\n",
      "Time taken to run training for epoch 7 : 23.90285849571228 \n",
      "Time taken to run training for epoch 7 : 0.5859718322753906 \n",
      "Total Time for epoch 8 is 24.488855123519897\n",
      "Training Loss for epoch 8 : 255.8883072733879 \n",
      "Training Accuracy for epoch 8 : 1891.3193359375\n",
      "Val Loss for epoch 8 : 25.26540493965149 \n",
      "Val Accuracy for epoch 8 : 1767.34619140625\n",
      "Train perplexity for epoch 8 : 1.351698501697061e+111\n",
      "Val perplexity for epoch 8 : 93891428869.69922\n",
      "starting Epoch : 9\n",
      "Time taken to run training for epoch 8 : 23.007503509521484 \n",
      "Time taken to run training for epoch 8 : 0.6125504970550537 \n",
      "Total Time for epoch 9 is 23.620131969451904\n",
      "Training Loss for epoch 9 : 256.67536893486977 \n",
      "Training Accuracy for epoch 9 : 1891.3572998046875\n",
      "Val Loss for epoch 9 : 25.3391672372818 \n",
      "Val Accuracy for epoch 9 : 1765.8077392578125\n",
      "Train perplexity for epoch 9 : 2.9695891577754486e+111\n",
      "Val perplexity for epoch 9 : 101078900034.76512\n",
      "starting Epoch : 10\n",
      "Time taken to run training for epoch 9 : 23.07025909423828 \n",
      "Time taken to run training for epoch 9 : 0.6193151473999023 \n",
      "Total Time for epoch 10 is 23.689650058746338\n",
      "Training Loss for epoch 10 : 254.96110254526138 \n",
      "Training Accuracy for epoch 10 : 1891.0538330078125\n",
      "Val Loss for epoch 10 : 24.99736624956131 \n",
      "Val Accuracy for epoch 10 : 1765.1539306640625\n",
      "Train perplexity for epoch 10 : 5.348105124939983e+110\n",
      "Val perplexity for epoch 10 : 71815505919.14688\n",
      "Saving the model \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    t1 = time.time()\n",
    "    print(\"starting Epoch : \" + str(epoch + 1))\n",
    "\n",
    "    train_samples = 0\n",
    "    val_samples = 0\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    train_accuracy = 0\n",
    "    t1 = time.time()\n",
    "    model.train()\n",
    "    for input_vector, label in train_dl:\n",
    "        \n",
    "\n",
    "        # add the input vector and label to the gpu\n",
    "        input_vector = input_vector.float()\n",
    "        input_vector = input_vector.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        logits,_ = model(input_vector)\n",
    "#         print(\"shape of the logits : {0}\",format(logits.shape))\n",
    "#         print(\"shape of the label : {0}\",format(label.shape))       \n",
    "        logits = logits.view(20* logits.shape[0], 9773)\n",
    "        label = label.view(-1)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(logits, label)\n",
    "\n",
    "        # set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # back.prop\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predictions = logits.argmax(dim=1)\n",
    "\n",
    "        # update accuracy\n",
    "        train_accuracy += (predictions == label).sum()\n",
    "\n",
    "        train_samples += input_vector.shape[0]\n",
    "        print(\"Progress : {0} \".format(train_samples * 100 / len(train_dl)),end='\\r')\n",
    "        \n",
    "    t2 = time.time()\n",
    "    print(\"Time taken to run training for epoch {0} : {1} \".format(\n",
    "        epoch, t2 - t1))\n",
    "    t3 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    for input_vector, label in val_dl:\n",
    "\n",
    "        # add the input vector and label to the gpu\n",
    "        input_vector = input_vector.float()\n",
    "        input_vector = input_vector.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        logits,_ = model(input_vector)\n",
    "        logits = logits.view(20* logits.shape[0], 9773)\n",
    "        label = label.view(-1)\n",
    "        \n",
    "\n",
    "        \n",
    "        # compute loss\n",
    "        loss = criterion(logits, label)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        predictions = logits.argmax(dim=1)\n",
    "\n",
    "        # update accuracy\n",
    "        val_accuracy += (predictions == label).sum()\n",
    "\n",
    "        val_samples += input_vector.shape[0]\n",
    "        print(\"Progress : {0} \".format(val_samples * 100 / len(val_dl)),end='\\r')\n",
    "        \n",
    "\n",
    "    t4 = time.time()\n",
    "    train_perplexity = np.exp(train_loss)\n",
    "    val_perplexity = np.exp(val_loss)\n",
    "    train_accuracy = train_accuracy / len(train_dl)\n",
    "    val_accuracy = val_accuracy / len(val_dl)\n",
    "    print(\"Time taken to run training for epoch {0} : {1} \".format(\n",
    "        epoch, t4 - t3))\n",
    "    print(\"Total Time for epoch {0} is {1}\".format(epoch+1, t4 - t1))\n",
    "    print(\"Training Loss for epoch {0} : {1} \".format(epoch + 1, train_loss))\n",
    "    print(\"Training Accuracy for epoch {0} : {1}\".format(\n",
    "        epoch + 1, train_accuracy))\n",
    "\n",
    "    print(\"Val Loss for epoch {0} : {1} \".format(epoch + 1, val_loss))\n",
    "    print(\"Val Accuracy for epoch {0} : {1}\".format(epoch + 1, val_accuracy))\n",
    "    print(\"Train perplexity for epoch {0} : {1}\".format(\n",
    "        epoch + 1, train_perplexity))\n",
    "    print(\"Val perplexity for epoch {0} : {1}\".format(\n",
    "        epoch + 1, val_perplexity))\n",
    "    error['train'].append(train_loss)\n",
    "    error['val'].append(val_loss)\n",
    "    accuracy['train'].append(train_accuracy)\n",
    "    accuracy['val'].append(val_accuracy)\n",
    "    times['train'].append(t2 - t1)\n",
    "    times['val'].append(t4-t3)\n",
    "    perplexity['train'].append(train_perplexity)\n",
    "    perplexity['val'].append(val_perplexity)\n",
    "\n",
    "    # log into wandb\n",
    "   \n",
    "\n",
    "    # save the model\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"Saving the model \")\n",
    "        torch.save(model.state_dict(),'./models/model_{0}.pt'.format(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bdd9074d09183e8bc0025300cef7a83eb88924cf0a158e90afa6b5f04cbbd876"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
