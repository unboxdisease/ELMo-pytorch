{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Formed and saved. The length of dictionary is-:  9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bi_RNN(\n",
       "  (lstm): LSTM(50, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=200, out_features=9773, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from dataset import Dataset_seq,build_vocab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import time\n",
    "from model import Bi_RNN,classifier\n",
    "\n",
    "train_path = './Dataset/yelp-subset.train.csv'\n",
    "word2id_ts,id2word_ts = build_vocab(train_path)\n",
    "model = Bi_RNN(50, 100, 100, 9773, 2)\n",
    "model.load_state_dict(torch.load('./models/model_20.pt'))\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from dataset import Dataset_seq,Dataset_class\n",
    "yelp_train = Dataset_seq(word2id_ts,id2word_ts, train_path)\n",
    "word2rep = yelp_train.word2representation\n",
    "# def csv_to_dataframe(path):\n",
    "df = pd.read_csv(train_path, sep=',', header=None)\n",
    "df.columns = ['label', 'text']\n",
    "df = df[1:]\n",
    "X = []\n",
    "Y = []\n",
    "for idx, row in enumerate(df.text):\n",
    "    sent = gensim.utils.simple_preprocess(df.iloc[idx].text,min_len=1)\n",
    "    sent = [word if word in yelp_train.word2id else '<unk>' for word in sent]\n",
    "    X.append([word2rep[w] for w in sent])\n",
    "    Y.append(df.iloc[idx].label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 50, got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(X)) : \n\u001b[1;32m      3\u001b[0m     b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray([X[i]]))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     out,lstm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     lstm \u001b[38;5;241m=\u001b[39m lstm\u001b[38;5;241m.\u001b[39mview(lstm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mlstm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],lstm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      6\u001b[0m     X1\u001b[38;5;241m.\u001b[39mappend(lstm)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Kushal/7sem/ANLP/ELMo-pytorch/model.py:33\u001b[0m, in \u001b[0;36mBi_RNN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#Forward pass through initial hidden layer\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# shape of self.hidden: (a, b), where a and b both\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# have shape (batch_size, num_layers, hidden_dim).\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     lstm_out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([lstm_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m100\u001b[39m],device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m     n1\u001b[38;5;241m=\u001b[39m lstm_out[:,\u001b[38;5;241m0\u001b[39m:lstm_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m100\u001b[39m] \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:767\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 767\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    770\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:692\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    688\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    689\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    690\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    691\u001b[0m                        ):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    694\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    696\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:205\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    203\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 50, got 0"
     ]
    }
   ],
   "source": [
    "X1 = []\n",
    "for i in range(1,len(X)) : \n",
    "    b = torch.FloatTensor(np.array([X[i]])).to('cuda')\n",
    "    out,lstm = model(b)\n",
    "    lstm = lstm.view(lstm.shape[0]*lstm.shape[1],lstm.shape[2])\n",
    "    X1.append(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb.init(project=\"ELMo\", entity=\"kushaljain\")\n",
    "lr = 0.0003\n",
    "epochs = 50\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = model.float()\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "error = {\"train\": [], \"val\": []}\n",
    "accuracy = {\"train\" : [], \"val\" : []}\n",
    "perplexity = {\"train\": [], \"val\": []}\n",
    "times = {\"train\": [], \"val\": []}\n",
    "wandb.config = {\n",
    "  \"learning_rate\": 0.003,\n",
    "  \"epochs\": 100,\n",
    "  \"batch_size\": 100\n",
    "}\n",
    "print(device)\n",
    "\n",
    "import sys\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    t1 = time.time()\n",
    "    print(\"starting Epoch : \" + str(epoch + 1))\n",
    "\n",
    "    train_samples = 0\n",
    "    val_samples = 0\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    train_accuracy = 0\n",
    "    t1 = time.time()\n",
    "    model.train()\n",
    "    for input_vector, label in train_dl:\n",
    "        \n",
    "\n",
    "        # add the input vector and label to the gpu\n",
    "        input_vector = input_vector.float()\n",
    "        input_vector = input_vector.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        logits,_ = model(input_vector)\n",
    "#         print(\"shape of the logits : {0}\",format(logits.shape))\n",
    "#         print(\"shape of the label : {0}\",format(label.shape))       \n",
    "        logits = logits.view(logits.shape[1]* logits.shape[0], logits.shape[2])\n",
    "        label = label.view(-1)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(logits, label)\n",
    "\n",
    "        # set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # back.prop\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predictions = logits.argmax(dim=1)\n",
    "\n",
    "        # update accuracy\n",
    "        train_accuracy += (predictions == label).sum()  \n",
    "        train_samples += logits.shape[0]\n",
    "#         print(train_accuracy)\n",
    "#         print(train_samples)\n",
    "        print(\"Progress : {0} \".format((train_samples/2000) * 100 / len(train_dl)),end='\\r')\n",
    "        \n",
    "    t2 = time.time()\n",
    "    print(\"Time taken to run training for epoch {0} : {1} \".format(\n",
    "        epoch, t2 - t1))\n",
    "    t3 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    for input_vector, label in val_dl:\n",
    "\n",
    "        # add the input vector and label to the gpu\n",
    "        input_vector = input_vector.float()\n",
    "        input_vector = input_vector.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        logits,_ = model(input_vector)\n",
    "        logits = logits.view(logits.shape[1]* logits.shape[0], logits.shape[2])\n",
    "        label = label.view(-1)\n",
    "        \n",
    "\n",
    "        \n",
    "        # compute loss\n",
    "        loss = criterion(logits, label)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        \n",
    "        # update accuracy\n",
    "        val_accuracy += (predictions == label).sum()\n",
    "\n",
    "        val_samples += logits.shape[0]\n",
    "        \n",
    "        print(\"Progress : {0} \".format((val_samples/2000) * 100 / len(val_dl)),end='\\r')\n",
    "        \n",
    "\n",
    "    t4 = time.time()\n",
    "    train_perplexity = np.exp(train_loss/len(train_dl))\n",
    "    val_perplexity = np.exp(val_loss/len(val_dl))\n",
    "    train_accuracy = train_accuracy / train_samples\n",
    "    val_accuracy = val_accuracy / val_samples\n",
    "    print(\"Time taken to run training for epoch {0} : {1} \".format(\n",
    "        epoch, t4 - t3))\n",
    "    print(\"Total Time for epoch {0} is {1}\".format(epoch+1, t4 - t1))\n",
    "    print(\"Training Loss for epoch {0} : {1} \".format(epoch + 1, train_loss))\n",
    "    print(\"Training Accuracy for epoch {0} : {1}\".format(\n",
    "        epoch + 1, train_accuracy))\n",
    "\n",
    "    print(\"Val Loss for epoch {0} : {1} \".format(epoch + 1, val_loss))\n",
    "    print(\"Val Accuracy for epoch {0} : {1}\".format(epoch + 1, val_accuracy))\n",
    "    print(\"Train perplexity for epoch {0} : {1}\".format(\n",
    "        epoch + 1, train_perplexity))\n",
    "    print(\"Val perplexity for epoch {0} : {1}\".format(\n",
    "        epoch + 1, val_perplexity))\n",
    "    error['train'].append(train_loss)\n",
    "    error['val'].append(val_loss)\n",
    "    accuracy['train'].append(train_accuracy)\n",
    "    accuracy['val'].append(val_accuracy)\n",
    "    times['train'].append(t2 - t1)\n",
    "    times['val'].append(t4-t3)\n",
    "    perplexity['train'].append(train_perplexity)\n",
    "    perplexity['val'].append(val_perplexity)\n",
    "\n",
    "    \n",
    "    wandb.log({\n",
    "        \"Training loss\": train_loss,\n",
    "        \"Validation loss\": val_loss,\n",
    "        \"Train accuracy\" : train_accuracy,\n",
    "        \"Validation accuracy\" : val_accuracy\n",
    "              })\n",
    "    \n",
    "    # save the model\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"Saving the model \")\n",
    "        torch.save(model.state_dict(),'./models/model_{0}.pt'.format(epoch + 1))\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bdd9074d09183e8bc0025300cef7a83eb88924cf0a158e90afa6b5f04cbbd876"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
